description: lora

target:
  service: amlk8s
  name: itp-scus-v100
  vc: AlexTScience

environment:
  image: wanglu315/traindte:1.9.0-cuda11.1-cudnn8-devel
  registry: docker.io

storage:
  biglearndatamodelpublic:
    storage_account_name: biglearndatamodel
    container_name: tscience-public
    mount_dir: /mount/biglearndatamodelpublic

code:
  # local directory of the code. this will be uploaded to the server.
  # $CONFIG_DIR is expanded to the directory of this config file
  local_dir: $CONFIG_DIR

search:
  job_template:
    # you may use {random_string:s} to avoid job name collisions
    # {auto:3s} generates lr_0.00000_mom_0.5, .. etc
    # {auto:2s} generates lr_0.00000_mo_0.5, .. etc
    name: hf_442_{auto:10s}
    sku: G8
    command:
    - . /opt/conda/bin/activate
    - conda deactivate
    - conda env create -f environment.yml
    - conda activate NLU
    - python3.7 -m pip install -e .
    - export num_gpus=8
    - export CUBLAS_WORKSPACE_CONFIG=":16:8" # https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility
    - export PYTHONHASHSEED=0
    - export output_dir="/mount/biglearndatamodelpublic/models/deberta_lora/glue/{task_name}/hf_442_{auto:10s}"
    - python3.7 -m torch.distributed.launch --nproc_per_node=$$num_gpus \
    - examples/text-classification/run_glue.py \
    - --model_name_or_path microsoft/{base_model} \
    - --task_name {task_name} \
    - --do_train \
    - --do_eval \
    - --max_seq_length 256 \
    - --per_device_train_batch_size {bs} \
    - --learning_rate {lr} \
    - --num_train_epochs {epoch} \
    - --output_dir $$output_dir/model \
    # - --overwrite_output_dir \
    - --logging_steps 10 \
    - --logging_dir $$output_dir/log \
    # - --deepspeed ds_config.json \
    - --fp16 \
    - --evaluation_strategy steps \
    - --eval_steps 500 \
    - --save_strategy steps \
    - --save_steps 500 \
    - --warmup_steps {warmup_steps} \
    - --cls_dropout {dropout} \
    - --apply_lora \
    - --lora_r {lora_r} \
    - --lora_alpha {lora_alpha} \
    - --seed {seed} \
    - --use_deterministic_algorithms {use_deterministic_algorithms}
  type: grid
  max_trials: 36
  params:
    - name: base_model
      spec: discrete
      values: ['deberta-v2-xxlarge']
    - name: task_name
      spec: discrete
      values: ['sst2', 'mrpc', 'cola', 'qnli', 'qqp', 'rte', 'stsb', 'wnli']
    - name: lr
      spec: discrete
      values: [1e-4, 8e-5, 5e-5, 3e-5]
    - name: dropout
      spec: discrete
      values: [0.0, 0.1, 0.15, 0.2, 0.3]
    - name: epoch
      spec: discrete
      values: [4, 5, 6]
    - name: bs
      spec: discrete
      values: [8, 14]
    - name: warmup_steps
      spec: discrete
      values: [1000]
    - name: lora_r
      spec: discrete
      values: [16]
    - name: lora_alpha
      spec: discrete
      values: [32]
    - name: seed
      spec: discrete
      values: [0]
    - name: use_deterministic_algorithms
      spec: discrete
      values: [true]
