{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: train e2e, src/gpt2_ft.py",
            "type": "python",
            "request": "launch",
            "program": "./src/gpt2_ft.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "cwd": "${workspaceFolder}/examples/NLG",
            "args": [
                "--train_data", "/home/avi/git_repos/LoRA/examples/NLG/data/e2e/train.jsonl",
                "--valid_data", "/home/avi/git_repos/LoRA/examples/NLG/data/e2e/valid.jsonl",
                "--train_batch_size", "4",
                "--grad_acc", "1",
                "--valid_batch_size", "4",
                "--seq_len", "512",
                "--model_card", "gpt2.md",
                "--init_checkpoint", "/home/avi/git_repos/LoRA/examples/NLG/pretrained_checkpoints/gpt2-medium-pytorch_model.bin",
                "--platform", "local",
                "--clip", "0.0",
                "--lr", "0.0002",
                "--weight_decay", "0.01",
                "--correct_bias",
                "--adam_beta2", "0.999",
                "--scheduler", "linear",
                "--warmup_step", "500",
                "--max_step", "20000", //added
                "--max_epoch", "5", // original
                //"--max_epoch", "2",
                "--save_interval", "1000",
                "--lora_dim", "4",
                "--lora_alpha", "32",
                "--lora_dropout", "0.1",
                "--label_smooth", "0.1",
                "--work_dir", "/home/avi/git_repos/LoRA/trained_models/GPT2_M/e2e",
                "--random_seed", "110"
            ]
        },
        {
            "name": "Generate outputs from the trained model using beam search",
            "type": "python",
            "request": "launch",
            "program": "./src/gpt2_beam.py",
            "cwd": "${workspaceFolder}/examples/NLG",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--data" ,"./data/e2e/test.jsonl",
                "--batch_size", "1",
                "--seq_len", "512",
                "--eval_len", "64",
                "--model_card", "gpt2.md",
                "--init_checkpoint", "/home/avi/git_repos/LoRA/trained_models/GPT2_M/e2e/gpt2_md_lora_e2e.pt",
                "--platform", "local",
                "--lora_dim", "4",
                "--lora_alpha", "32",
                "--beam", "10",
                "--length_penalty", "0.8",
                "--no_repeat_ngram_size", "4",
                "--repetition_penalty", "1.0",
                "--eos_token_id", "628",
                "--work_dir", "./trained_models/GPT2_M/e2e",
                "--output_file", "predict.26289.b10p08r4.jsonl"              
                    ]
        },
        {
            "name": "Python: deocde e2e, src/gpt2_decode.py",
            "type": "python",
            "request": "launch",
            "program": "./src/gpt2_decode.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "cwd": "${workspaceFolder}/examples/NLG",
            "args": [
                "--vocab", "./vocab",
                "--sample_file", "./trained_models/GPT2_M/e2e/predict.26289.b10p08r4.jsonl",
                "--input_file", "./data/e2e/test_formatted.jsonl",
                "--output_ref_file", "e2e_ref.txt",
                "--output_pred_file", "e2e_pred.txt"
            ]
        },
        {
            "name": "Python: eval/e2e/measure_scores.py e2e_ref.txt e2e_pred.txt -p",
            "type": "python",
            "request": "launch",
            "program": "./eval/e2e/measure_scores.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "cwd": "${workspaceFolder}/examples/NLG",            
            "args": [
                "e2e_ref.txt", 
                "e2e_pred.txt", 
                "-p"
                ]
        }
    ]
}

